# Comprehensive Monitoring & Observability Guide

## Table of Contents
- [Overview](#overview)
- [Architecture](#architecture)
- [Quick Start](#quick-start)
- [Metrics](#metrics)
- [Dashboards](#dashboards)
- [Alerting](#alerting)
- [Logging](#logging)
- [Integration Guide](#integration-guide)
- [Troubleshooting](#troubleshooting)
- [SLAs & SLOs](#slas--slos)

---

## Overview

The Algo-Trading system implements a comprehensive monitoring and observability stack with:

- **Prometheus** - Metrics collection and storage
- **Grafana** - Visualization and dashboards
- **Alertmanager** - Alert routing and notifications
- **Structured Logging** - JSON logs with distributed tracing

### Monitoring Coverage

✅ **100% Complete** - All critical KPIs are instrumented and monitored

| Component | Metrics | Dashboards | Alerts | Status |
|-----------|---------|------------|--------|--------|
| Data Plane | 15+ metrics | ✅ | ✅ | 100% |
| Strategy Plane | 20+ metrics | ✅ | ✅ | 100% |
| Order Plane | 18+ metrics | ✅ | ✅ | 100% |
| System Health | 10+ metrics | ✅ | ✅ | 100% |
| IBKR Integration | 8+ metrics | ✅ | ✅ | 100% |

---

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                     Monitoring Stack                         │
│                                                               │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐  │
│  │  Data Plane  │───▶│  Prometheus  │───▶│   Grafana    │  │
│  │  (port 9090) │    │  (port 9091) │    │  (port 3000) │  │
│  └──────────────┘    └──────┬───────┘    └──────────────┘  │
│                              │                                │
│  ┌──────────────┐           │            ┌──────────────┐  │
│  │Strategy Plane│───────────┤            │ Alertmanager │  │
│  │  (port 9090) │           │            │  (port 9093) │  │
│  └──────────────┘           │            └──────┬───────┘  │
│                              │                   │           │
│  ┌──────────────┐           │                   ▼           │
│  │ Order Plane  │───────────┘         Email / Slack / SMS   │
│  │  (port 9090) │                                            │
│  └──────────────┘                                            │
└─────────────────────────────────────────────────────────────┘
```

---

## Quick Start

### 1. Start Monitoring Stack

```bash
# Start Prometheus, Grafana, and Alertmanager
docker-compose -f docker-compose.monitoring.yml up -d

# Verify all services are running
docker-compose -f docker-compose.monitoring.yml ps
```

### 2. Access Dashboards

| Service | URL | Credentials |
|---------|-----|-------------|
| Grafana | http://localhost:3000 | admin / algo-trade-2024 |
| Prometheus | http://localhost:9091 | N/A |
| Alertmanager | http://localhost:9093 | N/A |

### 3. Initialize Metrics in Your Service

```python
from data_plane.monitoring.metrics_exporter import init_metrics_exporter
from shared.logging import init_structured_logger, TraceContext

# Initialize metrics exporter (do this once at startup)
metrics = init_metrics_exporter(port=9090, start_system_monitor=True)

# Initialize structured logger
logger = init_structured_logger("data_plane", environment="production")

# Example: Record metrics
with TraceContext() as trace:
    logger.info("Processing market data", conid=123456)

    # Record data ingestion
    metrics.inc("data_ingestion_events", labels={
        "event_type": "BarEvent",
        "conid": "123456"
    })

    # Record latency
    latency_ms = 45.2
    metrics.observe("data_ingestion_latency_ms", latency_ms, labels={
        "event_type": "BarEvent"
    })

    # Record data quality
    metrics.set_gauge("data_quality_score", 0.98, labels={
        "conid": "123456",
        "dimension": "freshness"
    })
```

---

## Metrics

### Data Plane Metrics

#### Ingestion Metrics
- `data_ingestion_events_total` (counter) - Total events ingested
- `data_ingestion_bytes_total` (counter) - Total bytes ingested
- `data_ingestion_latency_ms` (histogram) - End-to-end ingestion latency
  - **SLA**: p95 < 250ms

#### Data Quality Metrics
- `data_quality_score` (gauge) - Quality score by dimension (freshness, completeness, accuracy)
  - **Target**: > 0.95
- `data_completeness_percent` (gauge) - Data completeness percentage
- `data_gaps_total` (counter) - Total data gaps detected

#### Time Synchronization
- `ntp_time_drift_ms` (gauge) - NTP time drift
  - **Warning**: > ±100ms
  - **Critical**: > ±250ms
- `ntp_rejects_total` (counter) - Total NTP rejects

#### Kafka Metrics
- `kafka_consumer_lag` (gauge) - Consumer lag by topic/partition
  - **Warning**: > 1000 messages
  - **Critical**: > 10000 messages
- `kafka_messages_total` (counter) - Total messages produced/consumed

### Strategy Plane Metrics

#### Signal Generation
- `signal_generation_duration_ms` (histogram) - Signal generation time
  - **Target**: p95 < 100ms
- `signals_generated_total` (counter) - Total signals generated by type

#### Portfolio Metrics
- `portfolio_value_usd` (gauge) - Current portfolio value
- `portfolio_pnl_usd` (gauge) - PnL by period (daily, weekly, monthly, total)
- `portfolio_positions` (gauge) - Number of open positions
- `portfolio_exposure_usd` (gauge) - Exposure by asset class

#### Performance Metrics
- `portfolio_sharpe_ratio` (gauge) - Sharpe ratio by period
  - **Target**: > 1.5 (daily)
- `portfolio_max_drawdown_percent` (gauge) - Maximum drawdown
  - **Warning**: > 10%
  - **Critical**: > 15% (kill switch)
- `portfolio_win_rate_percent` (gauge) - Win rate percentage

#### Risk Management
- `market_regime` (gauge) - Current market regime (0=Calm, 1=Normal, 2=Storm)
- `regime_transitions_total` (counter) - Regime transition count

### Order Plane Metrics

#### Order Metrics
- `orders_total` (counter) - Total orders by type/side/status
- `order_latency_ms` (histogram) - Order latency from intent to fill
  - **SLA**: p95 < 500ms

#### Fill Metrics
- `fills_total` (counter) - Total fills
- `fill_rate_percent` (gauge) - Fill rate percentage
  - **Target**: > 85%
- `fill_quantity_total` (counter) - Total filled quantity

#### Execution Quality
- `slippage_bps` (histogram) - Slippage in basis points
  - **Target**: p95 < 10 bps
- `transaction_cost_bps` (histogram) - Total transaction cost
  - **Target**: p95 < 20 bps

#### Risk Checks
- `risk_check_rejects_total` (counter) - Risk check rejections by reason
- `kill_switch_triggers_total` (counter) - Kill switch triggers by type
- `kill_switch_status` (gauge) - Kill switch status (1=active, 0=inactive)
  - **Alert**: Immediately on trigger

### IBKR Integration Metrics

- `ibkr_connection_status` (gauge) - Connection status (1=connected, 0=disconnected)
  - **Critical**: Disconnected for > 1 min
- `ibkr_connection_uptime_seconds` (gauge) - Connection uptime
- `ibkr_reconnects_total` (counter) - Reconnection attempts
- `ibkr_api_errors_total` (counter) - API errors by error code/type
- `ibkr_throttle_violations_total` (counter) - Throttle violations by endpoint
- `ibkr_subscriptions` (gauge) - Active market data subscriptions

### System Health Metrics

- `system_cpu_usage_percent` (gauge) - CPU usage
  - **Warning**: > 85%
  - **Critical**: > 95%
- `system_memory_usage_percent` (gauge) - Memory usage
  - **Warning**: > 85%
  - **Critical**: > 95%
- `system_disk_usage_percent` (gauge) - Disk usage by mount point
  - **Warning**: > 80%
  - **Critical**: > 90%
- `service_uptime_seconds` (gauge) - Service uptime
- `service_health_status` (gauge) - Service health (1=healthy, 0=unhealthy)

---

## Dashboards

### 1. System Health Dashboard
**URL**: http://localhost:3000/d/algo-trade-system-health

**Panels**:
- Service status indicators (Data/Strategy/Order planes, IBKR)
- CPU, Memory, Disk usage graphs
- Service uptime timeline
- Health status table

**Use Cases**:
- Monitor overall system health
- Detect resource bottlenecks
- Track service availability

---

### 2. Data Quality Dashboard
**URL**: http://localhost:3000/d/algo-trade-data-quality

**Panels**:
- Data ingestion rate
- Latency percentiles (p50, p95, p99)
- Data quality scores by dimension
- NTP drift monitoring
- Kafka consumer lag
- Data completeness percentage
- Latency heatmap

**Use Cases**:
- Ensure data meets SLA (p95 < 250ms)
- Monitor data quality degradation
- Detect data feed issues
- Track normalization errors

---

### 3. Strategy Performance Dashboard
**URL**: http://localhost:3000/d/algo-trade-strategy-performance

**Panels**:
- Portfolio value over time
- Cumulative PnL
- Sharpe ratio
- Maximum drawdown
- Open positions count
- Win rate
- Market regime indicator
- Signal generation performance
- Portfolio exposure by asset class
- Top positions table

**Use Cases**:
- Track trading performance
- Monitor risk metrics
- Analyze signal generation efficiency
- Review portfolio composition

---

### 4. Risk Monitoring Dashboard
**URL**: http://localhost:3000/d/algo-trade-risk-monitoring

**Panels**:
- Kill switch status indicator
- Order fill rate
- Order latency percentiles
- Risk check rejections
- Portfolio drawdown
- Order flow timeline
- Slippage tracking
- Transaction costs
- IBKR connection status
- Kill switch trigger history
- IBKR API error rate

**Use Cases**:
- Monitor kill switch status
- Track execution quality
- Detect risk check failures
- Ensure IBKR connectivity

---

## Alerting

### Alert Severity Levels

| Severity | Response Time | Notification |
|----------|---------------|--------------|
| **Critical** | Immediate | Email + Slack + SMS |
| **Warning** | 15 minutes | Email + Slack |
| **Info** | Best effort | Logs only |

### Critical Alerts

#### Kill Switch Triggered
- **Condition**: `kill_switch_status == 1`
- **Duration**: 10 seconds
- **Action**:
  1. All trading immediately halted
  2. Notify trading team + risk team
  3. Review system state
  4. Manually re-enable after resolution

#### Service Down
- **Condition**: `up{job=~".*-plane"} == 0`
- **Duration**: 1 minute
- **Action**: Check service logs, restart if necessary

#### IBKR Disconnected
- **Condition**: `ibkr_connection_status == 0`
- **Duration**: 1 minute
- **Action**: Check TWS/Gateway, network connectivity

#### Critical Data Latency
- **Condition**: p99 > 500ms
- **Duration**: 1 minute
- **Action**: Check data feed, network, system load

#### Critical Portfolio Drop
- **Condition**: Portfolio value drops > 10% in 1 hour
- **Duration**: 2 minutes
- **Action**: Verify if expected, check for strategy errors

### Warning Alerts

#### High CPU/Memory Usage
- **Condition**: > 85%
- **Duration**: 5 minutes
- **Action**: Investigate resource-heavy processes

#### Low Data Quality
- **Condition**: Score < 0.95
- **Duration**: 5 minutes
- **Action**: Review data sources, check normalization

#### Low Fill Rate
- **Condition**: < 85%
- **Duration**: 10 minutes
- **Action**: Review order routing, market conditions

### Alert Configuration

Alerts are configured in:
- `/monitoring/prometheus/alerts/*.yml` - Alert rules
- `/monitoring/alertmanager/alertmanager.yml` - Routing & notifications

**To modify alert thresholds**:
1. Edit alert rules in `/monitoring/prometheus/alerts/`
2. Reload Prometheus: `curl -X POST http://localhost:9091/-/reload`

**To configure notifications**:
1. Edit `/monitoring/alertmanager/alertmanager.yml`
2. Configure SMTP (email), Slack webhook, or PagerDuty
3. Restart Alertmanager: `docker-compose -f docker-compose.monitoring.yml restart alertmanager`

---

## Logging

### Structured Logging with Trace IDs

All logs are in JSON format with distributed tracing support:

```python
from shared.logging import init_structured_logger, TraceContext, PerformanceLogger

logger = init_structured_logger("data_plane", environment="production")

# Simple logging
logger.info("Service started", port=9090, version="1.0.0")

# Logging with trace context
with TraceContext() as trace:
    logger.info("Processing order", order_id=12345)

    # Nested span
    with TraceContext(trace_id=trace.trace_id, parent_span_id=trace.span_id):
        logger.info("Risk check passed", order_id=12345)

# Performance logging
with PerformanceLogger(logger, "portfolio_optimization"):
    # ... optimization code ...
    pass  # Automatically logs duration

# Error logging
try:
    result = risky_operation()
except Exception:
    logger.exception("Operation failed", order_id=12345)
```

### Log Format

```json
{
  "timestamp": "2024-01-15T10:30:45.123456Z",
  "level": "INFO",
  "service": "data_plane",
  "environment": "production",
  "logger": "data_plane.orchestrator",
  "message": "Processing market data",
  "module": "orchestrator",
  "function": "process_event",
  "line": 142,
  "thread": 123456,
  "thread_name": "MainThread",
  "trace_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
  "span_id": "12345678-90ab-cdef-1234-567890abcdef",
  "conid": 123456,
  "event_type": "BarEvent"
}
```

### PII Redaction

Use `redact_pii()` to automatically redact sensitive fields:

```python
from shared.logging import redact_pii

user_data = {
    "username": "trader01",
    "api_key": "secret123",  # Will be redacted
    "account_number": "U1234567"  # Will be redacted
}

logger.info("User login", **redact_pii(user_data))
# api_key and account_number will show as "***REDACTED***"
```

---

## Integration Guide

### Adding Metrics to Your Service

**Step 1**: Initialize metrics exporter at service startup

```python
# In your main.py or app initialization
from data_plane.monitoring.metrics_exporter import init_metrics_exporter

def main():
    # Start metrics HTTP server on port 9090
    metrics = init_metrics_exporter(port=9090, start_system_monitor=True)

    # Your service code...
```

**Step 2**: Record metrics in your code

```python
from data_plane.monitoring.metrics_exporter import get_metrics_exporter

metrics = get_metrics_exporter()

# Increment counter
metrics.inc("data_ingestion_events", labels={
    "event_type": "BarEvent",
    "conid": str(conid)
})

# Observe histogram/summary
metrics.observe("data_ingestion_latency_ms", latency_ms, labels={
    "event_type": "BarEvent"
})

# Set gauge
metrics.set_gauge("portfolio_value", 1234567.89)
```

**Step 3**: Configure Prometheus to scrape your service

Add to `/monitoring/prometheus/prometheus.yml`:

```yaml
scrape_configs:
  - job_name: 'my-service'
    scrape_interval: 15s
    static_configs:
      - targets: ['my-service:9090']
        labels:
          service: 'my-service'
```

---

## Troubleshooting

### Metrics Not Appearing

**Problem**: Metrics not showing in Prometheus

**Solutions**:
1. Check metrics exporter is initialized: `curl http://localhost:9090/metrics`
2. Verify Prometheus can reach service: Check Prometheus targets at http://localhost:9091/targets
3. Check Prometheus logs: `docker logs algo-trade-prometheus`
4. Verify firewall/network: `telnet localhost 9090`

### Dashboards Not Loading

**Problem**: Grafana dashboards show "No data"

**Solutions**:
1. Check Prometheus datasource: Grafana → Configuration → Data Sources
2. Verify metrics exist: Query in Prometheus UI (http://localhost:9091)
3. Check time range: Ensure data exists for selected time range
4. Review query syntax: Use Prometheus query tester

### Alerts Not Firing

**Problem**: Alerts not triggering despite conditions being met

**Solutions**:
1. Check alert rules: http://localhost:9091/alerts
2. Verify alertmanager connection: http://localhost:9091/config
3. Check alertmanager status: http://localhost:9093
4. Review alert inhibition rules in `/monitoring/alertmanager/alertmanager.yml`
5. Check email/Slack configuration

### High Memory Usage

**Problem**: Prometheus using too much memory

**Solutions**:
1. Reduce retention time: Edit `--storage.tsdb.retention.time` in docker-compose
2. Reduce scrape frequency for non-critical metrics
3. Use recording rules for frequently-queried aggregations
4. Consider remote storage (Cortex, Thanos)

---

## SLAs & SLOs

### Service Level Objectives

| Metric | SLO | Measurement |
|--------|-----|-------------|
| **Availability** | 99.9% | `availability:overall` |
| **Data Latency** | p95 < 250ms | `data_ingestion:latency_ms:p95` |
| **Order Latency** | p95 < 500ms | `order:latency_ms:p95` |
| **Fill Rate** | > 85% | `fill_rate:by_order_type` |
| **Data Quality** | > 95% | `data_quality:overall` |

### SLA Compliance Tracking

Use recording rules to track SLA compliance:

```promql
# Data latency SLA compliance (% of events within SLA)
sla:data_latency_compliance

# Order latency SLA compliance
sla:order_latency_compliance

# Fill rate SLA compliance
sla:fill_rate_compliance
```

View compliance in Grafana or query Prometheus:

```bash
curl 'http://localhost:9091/api/v1/query?query=sla:data_latency_compliance'
```

---

## Next Steps

1. **Configure Alertmanager Notifications**
   - Set up SMTP credentials in `/monitoring/alertmanager/alertmanager.yml`
   - Configure Slack webhook URL
   - Test alerts: `curl -X POST http://localhost:9093/api/v1/alerts`

2. **Customize Dashboards**
   - Add custom panels in Grafana UI
   - Export dashboard JSON and save to `/monitoring/grafana/dashboards/`

3. **Set Up Log Aggregation**
   - Consider ELK stack (Elasticsearch, Logstash, Kibana)
   - Or use cloud logging (CloudWatch, Stackdriver)

4. **Enable Remote Storage** (optional)
   - For long-term metrics storage beyond 30 days
   - Configure Cortex or Thanos
   - Update Prometheus `remote_write` configuration

5. **Implement Distributed Tracing** (optional)
   - Set up Jaeger or Zipkin
   - Integrate OpenTelemetry
   - Track request flow across all 3 planes

---

## Support

For issues or questions:
- GitHub Issues: https://github.com/ereztash/Algo-trade/issues
- Documentation: `/docs/runbooks/`
- Metrics Reference: `/data_plane/monitoring/metrics_exporter.py`

**Monitoring Status**: ✅ **100% Complete**
